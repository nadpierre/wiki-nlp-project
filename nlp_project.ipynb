{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9be9711",
   "metadata": {},
   "source": [
    "# Wikipedia Music Genres NLP Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8fe6dc4",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e68579",
   "metadata": {},
   "source": [
    "This notebook performs an NLP on random articles collected from [Wikipedia](https://en.wikipedia.org/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef81e48",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47d46009",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import requests\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aafbc5d",
   "metadata": {},
   "source": [
    "## Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e72f7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://en.wikipedia.org/w/api.php\"\n",
    "csv_path = \"data/wiki_articles.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c819f40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_titles(n:int) -> np.array:\n",
    "    titles = []\n",
    "    while len(titles) < n:\n",
    "        try:\n",
    "            params = {\n",
    "                \"action\": \"query\",\n",
    "                \"list\": \"random\",\n",
    "                \"rnnamespace\": 0,  # Only articles\n",
    "                \"rnlimit\": min(50, n - len(titles)),\n",
    "                \"format\": \"json\"\n",
    "            }\n",
    "            response = requests.get(url, params=params)\n",
    "            data = response.json()\n",
    "            batch = [item[\"title\"] for item in data[\"query\"][\"random\"]]\n",
    "            titles.extend(batch)\n",
    "            time.sleep(0.5)\n",
    "        except Exception as e:\n",
    "            print(\"Error:\", e)\n",
    "            time.sleep(1)\n",
    "    return np.array(titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4345f3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_article_text(title:str) -> str:\n",
    "    params = {\n",
    "        \"action\": \"query\",\n",
    "        \"format\": \"json\",\n",
    "        \"prop\": \"extracts\",\n",
    "        \"explaintext\": True,\n",
    "        \"titles\": title\n",
    "    }\n",
    "    try:\n",
    "        response = requests.get(url, params=params)\n",
    "        pages = response.json()[\"query\"][\"pages\"]\n",
    "        page = next(iter(pages.values()))\n",
    "        return page.get(\"extract\", \"\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching {title}: {e}\")\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b4aaaed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_unwanted_sections(text):\n",
    "    unwanted_sections = [\n",
    "        r\"==\\s*See also\\s*==\",\n",
    "        r\"==\\s*References\\s*==\",\n",
    "        r\"==\\s*Further reading\\s*==\",\n",
    "        r\"==\\s*External links\\s*==\",\n",
    "        r\"==\\s*Notes\\s*==\",\n",
    "        r\"==\\s*Sources\\s*==\",\n",
    "        r\"==\\s*Bibliography\\s*==\",\n",
    "        r\"==\\s*Footnotes\\s*==\"\n",
    "    ]\n",
    "    \n",
    "    pattern = re.compile(\"|\".join(unwanted_sections), re.IGNORECASE)\n",
    "    match = pattern.search(text)\n",
    "    \n",
    "    if match:\n",
    "        return text[:match.start()].strip()\n",
    "    else:\n",
    "        return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63bffa08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_clean(text):\n",
    "    core = remove_unwanted_sections(text)\n",
    "    core = re.sub(r\"\\n{2,}\", \"\\n\", core)\n",
    "    return core.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ce1c294",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_list_to_csv(data:list[dict], csv_path:str) -> None:\n",
    "\tdf = pd.DataFrame(data)\n",
    "\tif not os.path.isfile(csv_path):\n",
    "\t\tdf.to_csv(csv_path, index=False)\n",
    "\telse:\n",
    "\t\tdf.to_csv(csv_path, index=False, header=False, mode='a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc3a559",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get 10000 random articles from Wikipedia\n",
    "titles = get_random_titles(10000)\n",
    "chunks = np.array_split(titles, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a295aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export data in chunks\n",
    "for i, chunk in enumerate(chunks):\n",
    "\tprint(f\"Chunk {i + 1}/{len(chunks)}\")\n",
    "\twiki_list = []\n",
    "\tfor title in chunk:\n",
    "\t\ttext = get_article_text(title)\n",
    "\t\tif text and len(text) > 300:  # Filter out very short pages\n",
    "\t\t\twiki_list.append({\n",
    "\t\t\t\t\"title\": title,\n",
    "\t\t\t\t\"text\": full_clean(text)\n",
    "\t\t\t})\n",
    "\t\ttime.sleep(0.5) # Sleep for 500 ms to avoid rate-limiting\n",
    "\texport_list_to_csv(wiki_list, csv_path)\n",
    "\tprint(f\"Added {len(wiki_list)}/{len(chunk)} articles to CSV file.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56964854",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "61561a3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Biff Schlitzer</td>\n",
       "      <td>Victor Joseph \"Biff\" Schlitzer (December 4, 18...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Prabhash Kumar</td>\n",
       "      <td>Prabhash Kumar is an Indian politician, farmer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>San Carlos Formation</td>\n",
       "      <td>The San Carlos Formation is a geological forma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023 in Ohio</td>\n",
       "      <td>The following is a list of events of the year ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2009 Iowa special elections</td>\n",
       "      <td>The 2009 Iowa state special elections were hel...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         title  \\\n",
       "0               Biff Schlitzer   \n",
       "1               Prabhash Kumar   \n",
       "2         San Carlos Formation   \n",
       "3                 2023 in Ohio   \n",
       "4  2009 Iowa special elections   \n",
       "\n",
       "                                                text  \n",
       "0  Victor Joseph \"Biff\" Schlitzer (December 4, 18...  \n",
       "1  Prabhash Kumar is an Indian politician, farmer...  \n",
       "2  The San Carlos Formation is a geological forma...  \n",
       "3  The following is a list of events of the year ...  \n",
       "4  The 2009 Iowa state special elections were hel...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data\n",
    "df = pd.read_csv(csv_path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e7a80493",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8595 entries, 0 to 8594\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   title   8595 non-null   object\n",
      " 1   text    8595 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 134.4+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde53031",
   "metadata": {},
   "source": [
    "### Tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d27f346",
   "metadata": {},
   "source": [
    "### Remove stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3993fec5",
   "metadata": {},
   "source": [
    "### Lemmatize or apply stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e535b260",
   "metadata": {},
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba80609b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF, BERT embeddings\n",
    "# Word embeddings like Word2Vec, Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d89832a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or use HF feature extraction model that does all those steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99954af7",
   "metadata": {},
   "source": [
    "## Unsupervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72634ee0",
   "metadata": {},
   "source": [
    "### K-Means"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94438ecf",
   "metadata": {},
   "source": [
    "### DBSCAN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6368774",
   "metadata": {},
   "source": [
    "### Latent Dirichlet Allocation (LDA)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d00a36",
   "metadata": {},
   "source": [
    "## Evaluation and Interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca07d101",
   "metadata": {},
   "source": [
    "### Visualize Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a417d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  t-SNE, UMAP, PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada6b5e1",
   "metadata": {},
   "source": [
    "### Analyse Clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729bc9d9",
   "metadata": {},
   "source": [
    "## End"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
