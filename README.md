# Wikipedia Pages NLP Project

## Description

The goal of this project is to uncover underlying clusters from a sample of Wikipedia pages, using unsupervized learning.

## Dataset

The data comes from the Wikipedia API named [MediaWiki](https://www.mediawiki.org/wiki/API:Main_page). The dataset contains 8595 titles and contents extracted from random pages. Each text contains at least 300 characters.

## Libraries Used

- gensim
- hdbscan
- kneed
- matplotlib
- numpy
- pandas
- requests
- scipy
- seaborn
- sklearn
- spacy
- pytorch
- transformers

## Methods & Models

https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2

## Results

## Featured Notebooks

[NLP Project](./nlp_project.ipynb)
